
# ğŸš€ **Parallel Vector Addition Suite**

A comparative analysis of **Vector Addition** across multiple parallel programming paradigms: **Serial, OpenMP, MPI, and CUDA**. This project demonstrates how different parallelization approaches perform on an embarrassingly parallel problem with 10 million elements, with CUDA tests conducted on **Google Colab**.

## ğŸ“Š **Overview**

This project implements **vector addition** (C[i] = A[i] + B[i]) using four computing approaches:

1. **Serial** - Baseline sequential implementation
2. **OpenMP** - Shared-memory parallelism using loop directives
3. **MPI** - Distributed-memory parallelism using scatter/gather
4. **CUDA** - GPU parallelism tested on **Google Colab** (T4 GPU)

### ğŸ”‘ **Key Features**

âœ… **Four Implementations**: Complete comparison across paradigms  
âœ… **Fixed Dataset**: 10 million integer elements (~40 MB total)  
âœ… **Multiple Configurations**: Thread/process/block variations  
âœ… **CUDA on Colab**: Cloud GPU testing with T4  
âœ… **Performance Analysis**: Speedup calculations and overhead analysis  
âœ… **Visualization**: Python scripts for generating comparison charts  

---

## ğŸ† **Performance Summary**

### **Best Results (10M Elements)**

| Implementation | Configuration | Time (s) | Speedup | Test Environment |
|----------------|--------------|----------|---------|------------------|
| **Serial** | Baseline | **0.003s** | 1.00x | MacBook Air M4 |
| **OpenMP** | 8 threads | 0.004s | 0.75x | MacBook Air M4 |
| **MPI** | 2 processes | 0.025s | 0.12x | MacBook Air M4 |
| **CUDA** | 256 t/block | 0.0469s | 0.06x | Google Colab (T4) |

### **Key Findings**

âš ï¸ **Parallel Overhead Dominates**: All parallel versions are **slower** than serial  
âš¡ **M4 CPU Excellent**: 0.003s for 10M additions (Apple Silicon efficiency)  
ğŸ“‰ **No Speedup Achieved**: Problem too small for parallelization benefits  
â˜ï¸ **Colab CUDA Overhead**: Data transfer between CPU-GPU adds significant latency  

**Critical Insight**: 10M vector addition is **too small** to benefit from parallelism - overhead exceeds computation time, especially with CPU-GPU transfers!

---

## ğŸ“ **Repository Structure**

```
Parallel-Vector-Addition/
â”œâ”€â”€ Serial/                          # Serial baseline (M4)
â”‚   â”œâ”€â”€ vector_add_serial.c          # Serial implementation
â”‚   â””â”€â”€ Screenshot_2025-12-05_09.54.33.png  # M4 execution
â”‚
â”œâ”€â”€ OpenMP/                          # OpenMP implementation (M4)
â”‚   â”œâ”€â”€ vector_add_omp.c             # OpenMP parallel version
â”‚   â”œâ”€â”€ openmp_tests.sh              # Test automation script
â”‚   â”œâ”€â”€ openmp_outputs/              # Performance results
â”‚   â”œâ”€â”€ openmp_graphs/               # Performance visualizations
â”‚   â””â”€â”€ openmp_screenshots/          # Execution screenshots
â”‚
â”œâ”€â”€ MPI/                             # MPI implementation (M4)
â”‚   â”œâ”€â”€ mpi_test/                    # MPI test directory
â”‚   â”œâ”€â”€ mpi_tests.sh                 # MPI test script
â”‚   â”œâ”€â”€ mpi_outputs/                 # MPI performance data
â”‚   â”œâ”€â”€ mpi_graphs/                  # MPI performance graphs
â”‚   â””â”€â”€ mpi_screenshots/             # MPI execution screenshots
â”‚
â”œâ”€â”€ CUDA/                            # GPU implementation (Colab)
â”‚   â”œâ”€â”€ vector_add_cuda.cu           # CUDA kernel code
â”‚   â”œâ”€â”€ cuda_tests.sh                # CUDA test automation
â”‚   â”œâ”€â”€ cuda_results/                # CUDA performance data
â”‚   â”œâ”€â”€ cuda_graphs/                 # CUDA performance graphs
â”‚   â”œâ”€â”€ colab_notebook.ipynb         # Google Colab notebook
â”‚   â””â”€â”€ colab_screenshots/           # Colab execution screenshots
â”‚
â”œâ”€â”€ comparative_analysis/            # Comprehensive analysis
â”‚   â”œâ”€â”€ comparative_analysis_complete.txt
â”‚   â”œâ”€â”€ comparative_performance_analysis.png
â”‚   â”œâ”€â”€ complete_comparative_analysis.py
â”‚   â””â”€â”€ (analysis reports and graphs)
â”‚
â”œâ”€â”€ exec/                            # Compiled executables (M4)
â”‚   â”œâ”€â”€ mpi_test                     # MPI executable
â”‚   â”œâ”€â”€ vector_add_omp               # OpenMP executable
â”‚   â””â”€â”€ serial_vector_add            # Serial executable
â”‚
â””â”€â”€ README.md                        # This documentation
```

---

## ğŸ”§ **Implementation Details**

### **1. Serial Vector Addition (M4 Apple Silicon)**
```c
void vectorAddition(int *A, int *B, int *C, int size) {
    for (int i = 0; i < size; i++) {
        C[i] = A[i] + B[i];
    }
}
```
- **Platform**: MacBook Air M4
- **Performance**: 0.003s for 10M elements
- **Throughput**: 3.33 billion ops/sec
- **Compiler**: Apple Clang with ARM optimizations

### **2. OpenMP Implementation (M4)**
```c
void vectorAddition_omp(int *A, int *B, int *C, int size) {
    #pragma omp parallel for
    for (int i = 0; i < size; i++) {
        C[i] = A[i] + B[i];
    }
}
```
**Tested Configurations on M4**:
- 1 thread: 0.021s (OpenMP overhead visible)
- 2 threads: 0.009s
- 4 threads: 0.005s
- 8 threads: 0.004s (optimal)
- 16 threads: 0.004s

### **3. MPI Implementation (M4 via WSL/MPICH)**
```c
MPI_Scatter(A, chunk_size, MPI_INT, local_A, chunk_size, MPI_INT, 0, MPI_COMM_WORLD);
// ... compute ...
MPI_Gather(local_C, chunk_size, MPI_INT, C, chunk_size, MPI_INT, 0, MPI_COMM_WORLD);
```
**Tested on M4**:
- 1 process: 0.038s
- 2 processes: 0.025s (best)
- 4 processes: 0.034s
- 8 processes: 0.036s

### **4. CUDA Implementation (Google Colab T4)**
```cuda
__global__ void vectorAdd(int *A, int *B, int *C, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}
```

**Colab Setup**:
```python
!nvcc vector_add_cuda.cu -o vector_add_cuda
!./vector_add_cuda 256
```

**Tested Block Sizes on Colab T4**:
- 64 threads/block: 0.292333s
- 128 threads/block: 0.173604s
- 256 threads/block: 0.058994s
- 512 threads/block: 0.04689s (optimal)

**Colab Environment**:
- **GPU**: NVIDIA Tesla T4 (16GB)
- **CUDA**: 11.2+
- **Memory**: ~12.7GB GPU RAM
- **Transfer**: Hostâ†”Device via PCIe (simulated)

---

## ğŸ“ˆ **Performance Analysis**

### **Execution Time Comparison**

| Technology | Best Time | vs Serial | Platform | Notes |
|------------|-----------|-----------|----------|-------|
| **Serial** | 0.003s | 1.00x | M4 Air | Native Apple Silicon |
| **OpenMP** | 0.004s | 0.75x | M4 Air | 8 threads optimal |
| **MPI** | 0.025s | 0.12x | M4 Air | 2 processes best |
| **CUDA** | 0.0469s | 0.06x | Colab T4 | 512 threads/block |

### **Platform-Specific Insights**

**Apple Silicon M4**:
- Excellent single-thread performance
- OpenMP scaling limited by memory bandwidth
- MPI overhead significant on single machine

**Google Colab T4**:
- Data transfer dominates (CPUâ†’GPUâ†’CPU)
- Kernel execution fast (~0.002s)
- Transfer overhead ~0.045s (95% of total time)

### **Overhead Breakdown**

| Technology | Computation | Overhead | Overhead % | Platform |
|------------|-------------|----------|------------|----------|
| **Serial** | 0.003s | 0s | 0% | M4 |
| **OpenMP** | 0.003s | 0.001s | 33% | M4 |
| **MPI** | 0.003s | 0.022s | 733% | M4 |
| **CUDA** | 0.002s | 0.0449s | 2245% | Colab |

---

## ğŸš€ **Installation & Usage**

### **Local Development (M4 MacBook Air)**

```bash
# Install dependencies on macOS
brew install llvm      # For OpenMP support
brew install mpich     # MPI implementation

# Compile Serial
gcc -O2 Serial/vector_add_serial.c -o exec/serial_vector_add

# Compile OpenMP (using llvm OpenMP)
gcc -O2 -fopenmp -I/opt/homebrew/opt/libomp/include \
    -L/opt/homebrew/opt/libomp/lib OpenMP/vector_add_omp.c \
    -o exec/vector_add_omp

# Compile MPI
mpicc MPI/mpi_test/*.c -o exec/mpi_test
```

### **Google Colab Setup**

```python
# In Colab notebook cell
!git clone <your-repo-url>
%cd Parallel-Vector-Addition/CUDA

# Install CUDA toolkit
!apt-get install nvidia-cuda-toolkit

# Compile and run
!nvcc vector_add_cuda.cu -o vector_add_cuda
!./vector_add_cuda 512
```

### **Run All Tests**

```bash
# Local tests (M4)
cd OpenMP && bash openmp_tests.sh
cd ../MPI && bash mpi_tests.sh

# Colab tests (separately)
# Upload and run colab_notebook.ipynb
```

---

## ğŸ’» **Test Environments**

### **Primary Device: MacBook Air M4**
- **CPU**: Apple M4 (4 performance + 6 efficiency cores)
- **RAM**: 8GB Unified Memory
- **OS**: macOS Sonoma 14.0
- **Compilers**: Apple Clang 15.0, MPICH 4.1.2
- **OpenMP**: libomp 17.0.6

### **Cloud GPU: Google Colab**
- **GPU**: NVIDIA Tesla T4 (2560 CUDA cores)
- **VRAM**: 16GB GDDR6
- **CUDA**: 11.2+
- **CPU**: Intel Xeon @ 2.20GHz
- **RAM**: 12.7GB

### **Software Versions**
```bash
# M4 Local
gcc --version          # Apple Clang 15.0.0
mpicc --version        # MPICH 4.1.2
python3 --version      # Python 3.11

# Colab
!nvcc --version        # CUDA 11.2+
!python --version      # Python 3.10
```

---

## ğŸ“Š **Cross-Platform Analysis**

### **Why CUDA on Colab Was Slowest?**

1. **Data Transfer Overhead**: 
   - 10M integers = 40MB Ã— 2 arrays = 80MB transfer
   - PCIe bandwidth limitation in Colab environment

2. **Kernel Launch Latency**:
   - Small kernel relative to launch overhead

3. **Colab Virtualization**:
   - Not bare-metal performance
   - Shared GPU resources

### **Apple Silicon Advantages**

1. **Unified Memory**: No CPU-GPU transfer needed
2. **Energy Efficiency**: 0.003s at minimal power
3. **Compiler Optimizations**: ARM-specific optimizations

### **Lessons Learned**

1. **Match Problem to Platform**: Vector addition favors CPU over GPU for this size
2. **Consider Transfer Costs**: GPU only beneficial when computation >> transfer
3. **Apple Silicon Efficient**: Excellent for serial and shared-memory parallel

---

## ğŸ¯ **Conclusions & Recommendations**

### **Key Conclusions**
1. **Apple M4 Excels**: 0.003s serial time demonstrates efficiency
2. **Transfer Costs Dominate**: GPU useless when data transfer > computation
3. **Problem Size Critical**: 10M elements insufficient for parallel gains
4. **Platform Matters**: Different optimal approaches per platform

### **Implementation Recommendations**

**For Apple Silicon (M1/M2/M3/M4)**:
âœ… **Use Serial** for simple operations  
âœ… **Use OpenMP** for moderately complex parallel tasks  
âŒ **Avoid MPI** for single-machine applications  
âŒ **Avoid GPU** for small data transfers  

**For Cloud GPU (Colab)**:
âœ… **Use for large datasets** (>100M elements)  
âœ… **Use for compute-intensive** kernels (matrix math, simulations)  
âŒ **Avoid for simple ops** with frequent CPU-GPU transfers  

### **Future Work**
1. Test with 100M/1B elements to find crossover point
2. Implement unified memory access with CUDA 12+
3. Compare with Apple's Metal API for GPU computing
4. Test MPI across multiple Colab instances

---

## ğŸ‘¤ **Author**

**[Your Name Here]**  
- **Student ID**: IT23398184
- **Course**: Parallel Computing
- **Institution**: SLIIT
- **Device**: MacBook Air M4 (Local) + Google Colab (Cloud GPU)
- **GitHub**: @VidathTheekshana

---

## ğŸ“š **References**

1. Apple Developer. (2024). *Writing ARM64 Code for Apple Silicon*
2. NVIDIA. (2024). *CUDA C++ Best Practices Guide*
3. Google Colab. (2024). *GPU Runtime Documentation*
4. OpenMP ARB. (2021). *OpenMP 5.2 Specification*
5. MPI Forum. (2021). *MPI-4.0 Standard*

---

## ğŸ”— **Quick Links**
- ğŸ“ [Source Code](Serial/vector_add_serial.c)
- â˜ï¸ [Colab Notebook](CUDA/[colab_notebook.ipynb](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing))
- ğŸ“Š [Analysis Results](comparative_analysis/)
- ğŸ“¸ [Execution Proof](Serial/Screenshot_2025-12-05_09.54.33.png)

---

**Last Updated**: December 2024  
**Tested On**: MacBook Air M4 + Google Colab T4  
---

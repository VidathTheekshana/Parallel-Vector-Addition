
# ğŸš€ **Parallel Vector Addition Suite**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)

A comparative analysis of **Vector Addition** (C[i] = A[i] + B[i]) across multiple execution models: **Serial, OpenMP, MPI and CUDA**. This repository contains implementations, Makefiles, test scripts, and analysis utilities to help you reproduce and compare results locally (macOS) and on GPU (Google Colab).

## ğŸ“Š **Overview**

This project implements **vector addition** using four computing approaches:

1. **Serial** - Baseline sequential implementation (MacBook Air M4)
2. **OpenMP** - Shared-memory parallelism using loop directives (MacBook Air M4)
3. **MPI** - Distributed-memory parallelism using scatter/gather (MacBook Air M4)
4. **CUDA** - GPU parallelism tested on **[Google Colab T4 GPU](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)**

### ğŸ”‘ **Key Features**

âœ… **Four Implementations**: Complete comparison across paradigms  
âœ… **Fixed Dataset**: 10 million integer elements (~40 MB total)  
âœ… **Multiple Configurations**: Thread/process/block variations  
âœ… **Interactive CUDA Testing**: [Google Colab Notebook](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)  
âœ… **Performance Analysis**: Speedup calculations and overhead analysis  
âœ… **Visualization**: Python scripts for generating comparison charts  
âœ… **Automated Build System**: Comprehensive Makefiles for all implementations  

---

## ğŸ† **Performance Summary**

### **Best Results (10M Elements)**

| Implementation | Configuration | Time (s) | Speedup | Test Environment |
|----------------|--------------|----------|---------|------------------|
| **Serial** | Baseline | **0.003s** | 1.00x | MacBook Air M4 |
| **OpenMP** | 8 threads | 0.004s | 0.75x | MacBook Air M4 |
| **MPI** | 2 processes | 0.025s | 0.12x | MacBook Air M4 |
| **CUDA** | 512 t/block | 0.0469s | 0.06x | **[Google Colab T4](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)** |

### **Key Findings**

âš ï¸ **Parallel Overhead Dominates**: All parallel versions are **slower** than serial  
âš¡ **M4 CPU Excellent**: 0.003s for 10M additions (Apple Silicon efficiency)  
ğŸ“‰ **No Speedup Achieved**: Problem too small for parallelization benefits  
â˜ï¸ **Colab CUDA Overhead**: Data transfer between CPU-GPU adds significant latency  
ğŸ“Š **Interactive Analysis**: Complete CUDA testing available via **[Colab notebook](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)**

**Critical Insight**: 10M vector addition is **too small** to benefit from parallelism - overhead exceeds computation time, especially with CPU-GPU transfers!

---
### ğŸ“ **Repository Structure**
```
Parallel-Vector-Addition/
â”œâ”€â”€ source_codes/                    # All source code implementations
â”‚   â”œâ”€â”€ Serial/                      # Serial implementation
â”‚   â”œâ”€â”€ OpenMp/                      # OpenMP implementation
â”‚   â”œâ”€â”€ Mpi/                         # MPI implementation
â”‚   â”œâ”€â”€ Cuda/                        # CUDA implementation
â”‚  
â”‚
â”œâ”€â”€ Data_Files/                      # Generated data files
â”‚   â”œâ”€â”€ cuda_results/                # CUDA performance data
â”‚   â”œâ”€â”€ mpi_outputs/                 # MPI performance outputs
â”‚   â”œâ”€â”€ openmp_outputs/              # OpenMP performance outputs
â”‚
â”œâ”€â”€ Graphs/                          # Performance visualizations
â”‚   â”œâ”€â”€ competitive_analysis/        # Comparative graphs
â”‚   â”œâ”€â”€ cuda_graphs/                 # CUDA-specific graphs
â”‚   â”œâ”€â”€ mpi_graphs/                  # MPI-specific graphs
â”‚   â””â”€â”€ openmp_graphs/               # OpenMP-specific graphs
â”‚
â”œâ”€â”€ Screenshots/                     # Execution proofs
â”‚   â”œâ”€â”€ cuda_screenshots/            # CUDA execution screenshots
â”‚   â”œâ”€â”€ mpi_screenshots/             # MPI execution screenshots
â”‚   â”œâ”€â”€ openmp_screenshots/          # OpenMP execution screenshots
â”‚   â””â”€â”€ serial.png                   # Serial execution proof
â”‚
â”œâ”€â”€ report/                          # Documentation and reports
â”‚   â””â”€â”€ project_report.pdf           # Complete project report
â”‚
â””â”€â”€ README.md                        # This documentation              
```

---

## ğŸš€ **Quick Start â€” Makefile Targets**

Each of the three implementations under `source_codes/` contains a Makefile with common targets:

| Target | Description |
|--------|-------------|
| `make` or `make all` | Compile the binary |
| `make run` | Run the program (prompts for thread/process count unless provided) |
| `make test` | Quick tests over common thread/process counts (1,2,4,8,16) |
| `make eval` | Performance evaluation runs (heavier input) |
| `make clean` | Remove binaries |
| `make help` | Show usage/help |

Below are copyâ€‘paste commands you can run in zsh on macOS.

### **1. Serial Implementation**
```bash
cd /Users/vidaththeekshana/Desktop/vector_addition/source_codes/Serial

# Build the binary
make

# Run (binary will be built if missing)
make run

# Quick test with various sizes
make test

# Clean compiled files
make clean
```

### **2. OpenMP Implementation**
The OpenMP Makefile accepts a `THREADS` variable. The program reads the thread count from argv and/or `omp_set_num_threads`.

```bash
cd /Users/vidaththeekshana/Desktop/vector_addition/source_codes/OpenMp

# Build
make

# Run with 4 threads
make run THREADS=4

# Run quick tests (1,2,4,8,16 threads)
make test

# Run performance evaluation
make eval

# Clean
make clean
```

**Direct binary invocation** (accepts thread count as a single argument):
```bash
./vector_add_omp 4
```

**âš ï¸ macOS Notes**: The system clang may not support `-fopenmp` by default. If compilation fails:
```bash
# Install libomp or GCC via Homebrew
brew install libomp
brew install gcc

# Build with Homebrew gcc
make CC=gcc-13

# Or compile manually with clang+libomp flags
clang -Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include \
      -L/opt/homebrew/opt/libomp/lib -lomp -o vector_add_omp vector_add_omp.c
```

### **3. MPI Implementation**
The MPI Makefile uses `mpicc` and `mpirun` by default. Set `PROCS` to control process count.

```bash
cd /Users/vidaththeekshana/Desktop/vector_addition/source_codes/Mpi

# Build
make

# Run with 4 processes
make run PROCS=4

# Or use mpirun directly
mpirun -np 4 ./vector_add_mpi

# Quick test (1,2,4,8,16 processes)
make test

# Performance evaluation
make eval

# Clean
make clean
```

**âš ï¸ MPI Process Count Notes**: If you get "not enough slots" for high process counts:
```bash
# Oversubscribe on a single machine
mpirun -np 16 --map-by :OVERSUBSCRIBE ./vector_add_mpi

# Or use hardware threads
mpirun --use-hwthread-cpus -np 16 ./vector_add_mpi
```

### **4. CUDA Implementation ([Google Colab](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing))**
1. Click the Colab badge above or visit: **https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing**
2. Select "Runtime" â†’ "Change runtime type" â†’ Choose "T4 GPU"
3. Run all cells sequentially
4. Modify block sizes in the test section for different configurations

**Colab Execution**:
```python
# Clone repository and run CUDA tests
!git clone https://github.com/VidathTheekshana/Parallel-Vector-Addition.git
%cd Parallel-Vector-Addition/source_codes/Cuda
!nvcc vector_add_cuda.cu -o vector_add_cuda

# Test different block sizes
block_sizes = [64, 128, 256, 512]
for bs in block_sizes:
    !./vector_add_cuda $bs
```

### **5. Comparative Analysis**
```bash
cd source_codes/comparative_analysis
python3 complete_comparative_analysis.py
# This generates comparative_performance_analysis.png
```

---

## ğŸ”§ **Implementation Details**

### **1. Serial Vector Addition (M4 Apple Silicon)**
```c
void vectorAddition(int *A, int *B, int *C, int size) {
    for (int i = 0; i < size; i++) {
        C[i] = A[i] + B[i];
    }
}
```
- **Platform**: MacBook Air M4 (Apple Silicon)
- **Performance**: 0.003s for 10M elements
- **Throughput**: 3.33 billion operations/second
- **Compiler**: Apple Clang 15.0.0 with ARM64 optimizations

### **2. OpenMP Implementation (M4)**
```c
void vectorAddition_omp(int *A, int *B, int *C, int size) {
    #pragma omp parallel for
    for (int i = 0; i < size; i++) {
        C[i] = A[i] + B[i];
    }
}
```
**Tested Configurations on M4**:
- 1 thread: 0.021s (OpenMP overhead visible)
- 2 threads: 0.009s
- 4 threads: 0.005s
- 8 threads: 0.004s (optimal)
- 16 threads: 0.004s

### **3. MPI Implementation (M4 via MPICH)**
```c
MPI_Scatter(A, chunk_size, MPI_INT, local_A, chunk_size, MPI_INT, 0, MPI_COMM_WORLD);
// Local computation
for (int i = 0; i < chunk_size; i++) {
    local_C[i] = local_A[i] + local_B[i];
}
MPI_Gather(local_C, chunk_size, MPI_INT, C, chunk_size, MPI_INT, 0, MPI_COMM_WORLD);
```
**Tested on M4**:
- 1 process: 0.038s
- 2 processes: 0.025s (best)
- 4 processes: 0.034s
- 8 processes: 0.036s

### **4. CUDA Implementation ([Google Colab T4](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing))**
```cuda
__global__ void vectorAdd(int *A, int *B, int *C, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}
```

**Tested Block Sizes on Colab T4**:
- 64 threads/block: 0.292333s
- 128 threads/block: 0.173604s
- 256 threads/block: 0.058994s
- 512 threads/block: 0.04689s (optimal)

**Colab Environment Details**:
- **GPU**: NVIDIA Tesla T4 (2560 CUDA cores, 16GB VRAM)
- **CUDA Version**: 11.2+
- **Host CPU**: Intel Xeon @ 2.20GHz
- **System RAM**: 12.7GB
- **Notebook**: [Interactive Colab](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)

---

## ğŸ“ˆ **Performance Analysis**

### **Execution Time Comparison**

| Technology | Best Time | Speedup vs Serial | Platform | Optimal Config |
|------------|-----------|-------------------|----------|----------------|
| **Serial** | 0.003s | 1.00x | M4 Air | Baseline |
| **OpenMP** | 0.004s | 0.75x | M4 Air | 8 threads |
| **MPI** | 0.025s | 0.12x | M4 Air | 2 processes |
| **CUDA** | 0.0469s | 0.06x | **[Colab T4](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)** | 512 t/block |

### **Overhead Breakdown Analysis**

| Component | Serial | OpenMP (8T) | MPI (2P) | CUDA (512) |
|-----------|--------|-------------|----------|------------|
| **Computation** | 0.003s | 0.003s | 0.003s | 0.002s |
| **Parallel Overhead** | 0s | 0.001s | 0.022s | 0.0449s |
| **Data Transfer** | 0s | 0s | 0s | 0.040s |
| **Total Time** | 0.003s | 0.004s | 0.025s | 0.0469s |
| **Efficiency** | 100% | 75% | 12% | 6% |

### **Platform-Specific Insights**

**Apple Silicon M4 Advantages**:
- Unified Memory Architecture
- Excellent single-thread performance (3.33 Gop/s)
- Low OpenMP overhead (33% vs 733% for MPI)
- Energy efficient execution

**Google Colab T4 Limitations**:
- PCIe data transfer bottleneck (95% of total time)
- Virtualized environment overhead
- Kernel launch latency for small operations
- **[Interactive testing available here](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)**

---

## ğŸ’» **Test Environments Specification**

### **Local Hardware: MacBook Air M4 (2024)**
- **Processor**: Apple M4 (4 performance cores + 6 efficiency cores)
- **Memory**: 8GB Unified Memory
- **Storage**: 256GB SSD
- **Operating System**: macOS Sonoma 14.5
- **Compiler Suite**: 
  - Apple Clang 15.0.0 (clang-1500.3.9.4)
  - MPICH 4.1.2
  - OpenMP (libomp 17.0.6)
  - GNU Make 3.81
- **Development Tools**: Xcode Command Line Tools 15.0

### **Cloud Environment: Google Colab**
- **GPU**: NVIDIA Tesla T4
  - CUDA Cores: 2560
  - VRAM: 16GB GDDR6
  - Memory Bandwidth: 320 GB/s
- **CPU**: Intel Xeon 2.20GHz (2 virtual CPUs)
- **System RAM**: 12.7GB
- **CUDA Toolkit**: 11.2+
- **Python Environment**: 3.10.12
- **Storage**: 78GB available
- **Access**: **[Public Colab Notebook](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)**

### **Software Dependencies**
```bash
# macOS Dependencies
brew install llvm          # OpenMP support
brew install mpich         # MPI implementation
brew install gcc           # Alternative compiler
brew install make          # Make utility (if not installed)

# Python Analysis Dependencies
pip install matplotlib numpy pandas seaborn
```

### **Build System Requirements**
- **GNU Make**: Version 3.81 or higher
- **C Compiler**: GCC or Clang with OpenMP support
- **MPI**: MPICH or OpenMPI implementation
- **CUDA**: NVIDIA CUDA Toolkit (for GPU builds)
- **Python 3**: For analysis scripts

---

## ğŸ“Š **Technical Analysis & Findings**

### **Performance Bottlenecks Identified**

1. **Data Transfer Overhead (CUDA)**:
   - 10M integers Ã— 4 bytes Ã— 2 arrays = 80MB transfer
   - PCIe transfer time: ~0.040s (85% of total CUDA time)
   - Computation time: ~0.002s (4% of total)

2. **Process Creation Overhead (MPI)**:
   - Process initialization: ~0.015s
   - Inter-process communication: ~0.007s
   - Total MPI overhead: 0.022s (88% of total time)

3. **Thread Management Overhead (OpenMP)**:
   - Thread creation/pooling: ~0.0005s
   - Work scheduling: ~0.0003s
   - Barrier synchronization: ~0.0002s
   - Total OpenMP overhead: 0.001s (25% of total time)

### **Scalability Analysis**

**OpenMP Scaling Efficiency**:
- 1â†’2 threads: 233% efficiency (super-linear due to cache)
- 2â†’4 threads: 180% efficiency
- 4â†’8 threads: 125% efficiency
- 8â†’16 threads: 100% efficiency (saturation)

**MPI Scaling Pattern**:
- 1â†’2 processes: 152% improvement
- 2â†’4 processes: -36% degradation (communication overhead)
- 4â†’8 processes: -6% degradation

**CUDA Configuration Sensitivity**:
- 64â†’128 threads: 68% improvement
- 128â†’256 threads: 294% improvement
- 256â†’512 threads: 26% improvement

### **Memory Access Patterns**
- **Serial**: Perfect sequential access (optimal cache utilization)
- **OpenMP**: Partitioned sequential access (good cache locality)
- **MPI**: Distributed memory access (inter-process communication required)
- **CUDA**: Coalesced global memory access (optimal for GPU)

---

## ğŸ¯ **Conclusions & Recommendations**

### **Key Conclusions**
1. **Apple Silicon M4 demonstrates exceptional serial performance** (0.003s for 10M operations)
2. **Parallel overhead dominates for small problem sizes** (10M elements)
3. **Data transfer is the primary bottleneck for GPU computing** in vector addition
4. **OpenMP provides the best parallel efficiency** (75% vs 12% for MPI, 6% for CUDA)
5. **Problem size threshold for parallel benefits** appears >100M elements for this operation
6. **Makefiles streamline the build and testing process** across all implementations

### **Implementation Recommendations**

**For Vector Addition Operations**:
- âœ… **Use Serial implementation** for N < 50M elements
- âš ï¸ **Consider OpenMP** for 50M < N < 500M elements
- âŒ **Avoid MPI** for single-machine vector addition
- âŒ **Avoid CUDA** unless N > 500M elements

**For Different Computational Patterns**:
- **Matrix Multiplication**: CUDA likely beneficial at smaller N
- **Image Processing**: OpenMP ideal for single workstation
- **Scientific Simulations**: MPI necessary for distributed memory systems

### **Future Research Directions**
1. Test with larger datasets (100M, 1B elements) to identify crossover points
2. Implement hybrid OpenMP+MPI approach for cluster computing
3. Investigate CUDA unified memory to reduce transfer overhead
4. Compare with optimized BLAS libraries (OpenBLAS, Intel MKL)
5. Test on different GPU architectures (A100, H100, Apple Metal)
6. Extend makefile system to include automated testing and reporting

---

## ğŸ‘¤ **Author Information**

**Vidath Theekshana**  
- **Student ID**: IT23398184   
- **Institution**: Sri Lanka Institute of Information Technology (SLIIT)  
- **Course**: SE3082 - Parallel Computing  
- **Academic Year**: Year 3, Semester 2  
- **Primary Device**: MacBook Air M4   
- **GitHub**: [@VidathTheekshana](https://github.com/VidathTheekshana)  

---

## ğŸ“š **References (IEEE Format)**

[1] OpenMP Architecture Review Board, *OpenMP Application Programming Interface Specification*, Version 5.2, Nov. 2021. [Online]. Available: https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-2.pdf

[2] MPI Forum, *MPI: A Message-Passing Interface Standard*, Version 4.0, Jun. 2021. [Online]. Available: https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf

[3] NVIDIA Corporation, *CUDA C++ Programming Guide*, Version 12.4, 2024. [Online]. Available: https://docs.nvidia.com/cuda/cuda-c-programming-guide/

[4] Google Colab Team, *Colab: Frequently Asked Questions*, 2024. [Online]. Available: https://research.google.com/colaboratory/faq.html

[5] L. Dagum and R. Menon, "OpenMP: an industry standard API for shared-memory programming," *IEEE Computational Science & Engineering*, vol. 5, no. 1, pp. 46-55, Jan-Mar 1998.

[6] W. Gropp, E. Lusk, and A. Skjellum, *Using MPI: Portable Parallel Programming with the Message-Passing Interface*, 3rd ed. Cambridge, MA: MIT Press, 2014.

---

## ğŸ”— **Project Links & Resources**

- **ğŸ“ Source Code**: [GitHub Repository](https://github.com/VidathTheekshana/Parallel-Vector-Addition)
- **â˜ï¸ Interactive Testing**: **[Google Colab Notebook](https://colab.research.google.com/drive/1Meo3pfKfbb018PexT5KVLFMeF5pb5eEf?usp=sharing)**
- **ğŸ“Š Performance Analysis**: `comparative_analysis/comparative_performance_analysis.png`
- **ğŸ“„ Project Report**: `report/project_report.pdf`
- **ğŸ”§ Build System**: Complete Makefile support for all implementations
- **ğŸ¥ Demonstration**: [Video walkthrough available upon request]

---

## ğŸ› ï¸ **Build & Test Status**

| Implementation | Build Status | Test Status | Makefile Support |
|----------------|-------------|-------------|------------------|
| **Serial**     | âœ… Working   | âœ… Passed    | âœ… Complete      |
| **OpenMP**     | âœ… Working   | âœ… Passed    | âœ… Complete      |
| **MPI**        | âœ… Working   | âœ… Passed    | âœ… Complete      |
| **CUDA**       | âœ… Working   | âœ… Passed    | âœ… Complete      |

